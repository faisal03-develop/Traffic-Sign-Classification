# -*- coding: utf-8 -*-
"""Traffic_sign_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LF5Hc4f4eYBZFapDAW_yVj0PQIYL8U7d
"""

!pip install opendatasets

import opendatasets as od

od.download("https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten , Conv2D , MaxPooling2D , Dropout, BatchNormalization
from tensorflow.keras.utils import  to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from PIL import Image
import cv2 as cv
import os
from sklearn.model_selection import train_test_split

dataset_path = "/content/gtsrb-german-traffic-sign/Train"
X_train = []
y_train = []
num_corrupted = 0

for label in os.listdir(dataset_path):
    folder_path = os.path.join(dataset_path, label)
    if not os.path.isdir(folder_path):
        continue

    for img_name in os.listdir(folder_path):
        if not img_name.lower().endswith(".png"):
            continue
        img_path = os.path.join(folder_path, img_name)
        try:
            img = cv.imread(img_path)
            if img is None:
                num_corrupted += 1
                continue
            img = cv.resize(img, (64, 64))
            X_train.append(img)
            y_train.append(int(label))
        except:
            num_corrupted += 1

X_train = np.array(X_train)
y_train = np.array(y_train)

print(f"Number of corrupted images: {num_corrupted}")
print(f"Number of loaded images: {len(X_train)}")
print(f"Number of labels: {len(y_train)}")

base_path = "/content/gtsrb-german-traffic-sign"
csv_path = os.path.join(base_path, "Test.csv")

df = pd.read_csv(csv_path)

X_test = []
y_test = []
num_corrupted = 0

for idx, row in df.iterrows():
    img_rel_path = row["Path"]
    label = row["ClassId"]

    img_path = os.path.join(base_path, img_rel_path)
    try:
        img = cv.imread(img_path)
        if img is None:
            num_corrupted += 1
            continue
        img = cv.resize(img, (64, 64))
        X_test.append(img)
        y_test.append(label)
    except:
        num_corrupted += 1

X_test = np.array(X_test)
y_test = np.array(y_test)

print(f"Number of corrupted test images: {num_corrupted}")
print(f"Number of loaded test images: {len(X_test)}")
print(f"Number of test labels: {len(y_test)}")

num_classes = len(np.unique(y_train))
print("Number of classes:", num_classes)

X_train, X_validation, y_train, y_validation = train_test_split(
    X_train, y_train, test_size=0.001, random_state=42, shuffle=True
)

X_train = np.array(X_train) / 255.0
X_test = np.array(X_test) / 255.0
X_validation = np.array(X_validation) / 255.0

X_train = X_train.reshape(-1, 64, 64, 3)
X_test = X_test.reshape(-1, 64, 64, 3)
X_validation = X_validation.reshape(-1, 64, 64, 3)

plt.figure(figsize=(30, 3))

for index, (image, label) in enumerate(zip(X_train[0:6], y_train[0:6])):
    plt.subplot(1, 6, index + 1)
    image = image.reshape(64, 64, 3)
    plt.imshow(image)
    plt.title(f"Label: {label}")
    plt.axis('off')

plt.show()

model = Sequential([
    Conv2D(32 , (3,3), activation='relu', input_shape=(64,64,3)),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(64, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(128, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(43, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)

history  = model.fit(
    X_train, y_train,
    epochs = 15,
    batch_size = 128,
    validation_data=(X_validation, y_validation),
    callbacks=[early_stop_cb]
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']


epochs = range(1, len(acc) + 1)


plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, loss, 'ro', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

predictions = model.predict(X_test)

results = model.evaluate(X_test, y_test)
print('Test loss, Test Accuracy:', results)

# Select a random image from the test set
random_index = np.random.randint(0, len(X_test))
unseen_image = X_test[random_index]
true_label = y_test[random_index]

# Reshape the image for prediction (add a batch dimension)
unseen_image_reshaped = unseen_image.reshape(1, 64, 64, 3)

# Make a prediction
prediction = model.predict(unseen_image_reshaped)
predicted_class = np.argmax(prediction)

# Display the image and the prediction result
plt.imshow(unseen_image)
plt.title(f"Predicted Class: {predicted_class}\nTrue Label: {true_label}")
plt.axis('off')
plt.show()

print(f"Model predicted class: {predicted_class}")
print(f"True class label: {true_label}")

# You can also check if the prediction is correct
if predicted_class == true_label:
    print("Prediction is correct!")
else:
    print("Prediction is incorrect.")

model_modern = Sequential([
    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    Conv2D(256, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    Flatten(),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(num_classes, activation='softmax') # Use num_classes from previous cell
])

model_modern.summary()

model_modern.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)

history_modern = model_modern.fit(
    X_train, y_train,
    epochs=15,
    batch_size=128,
    validation_data=(X_validation, y_validation),
    callbacks=[early_stop_cb]
)

# Evaluate the modern model on the test set
results_modern = model_modern.evaluate(X_test, y_test)

print("Previous model test accuracy:", results[1])
print("Modern model test accuracy:", results_modern[1])

# Optional: You can also compare the loss
# print("Previous model test loss:", results[0])
# print("Modern model test loss:", results_modern[0])

# Get the history data for both models
acc_prev = history.history['accuracy']
val_acc_prev = history.history['val_accuracy']
loss_prev = history.history['loss']
val_loss_prev = history.history['val_loss']

acc_modern = history_modern.history['accuracy']
val_acc_modern = history_modern.history['val_accuracy']
loss_modern = history_modern.history['loss']
val_loss_modern = history_modern.history['val_loss']

# Create epochs range for both models
epochs_prev = range(1, len(acc_prev) + 1)
epochs_modern = range(1, len(acc_modern) + 1)

plt.figure(figsize=(12, 5))

# Plot training and validation accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_prev, acc_prev, 'bo', label='Previous Training acc')
plt.plot(epochs_prev, val_acc_prev, 'b', label='Previous Validation acc')
plt.plot(epochs_modern, acc_modern, 'ro', label='Modern Training acc')
plt.plot(epochs_modern, val_acc_modern, 'r', label='Modern Validation acc')
plt.title('Training and validation accuracy comparison')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(epochs_prev, loss_prev, 'bo', label='Previous Training loss')
plt.plot(epochs_prev, val_loss_prev, 'b', label='Previous Validation loss')
plt.plot(epochs_modern, loss_modern, 'ro', label='Modern Training loss')
plt.plot(epochs_modern, val_loss_modern, 'r', label='Modern Validation loss')
plt.title('Training and validation loss comparison')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Get predictions for both models
predictions_prev = model.predict(X_test)
predicted_classes_prev = np.argmax(predictions_prev, axis=1)

predictions_modern = model_modern.predict(X_test)
predicted_classes_modern = np.argmax(predictions_modern, axis=1)

# Generate confusion matrices
cm_prev = confusion_matrix(y_test, predicted_classes_prev)
cm_modern = confusion_matrix(y_test, predicted_classes_modern)

# Plot confusion matrices
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.heatmap(cm_prev, annot=False, cmap='Blues')
plt.title('Confusion Matrix (Previous Model)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

plt.subplot(1, 2, 2)
sns.heatmap(cm_modern, annot=False, cmap='Blues')
plt.title('Confusion Matrix (Modern Model)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

plt.tight_layout()
plt.show()

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define data augmentation steps
datagen = ImageDataGenerator(
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    horizontal_flip=False, # Keep False for traffic signs
    vertical_flip=False,   # Keep False for traffic signs
    fill_mode='nearest'
)

# Fit the data augmentation generator on the training data
datagen.fit(X_train)

# Compile the previous model again (if needed, but it was already compiled)
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the previous model with data augmentation
early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)

history_augmented = model.fit(
    datagen.flow(X_train, y_train, batch_size=128),
    epochs=15,
    validation_data=(X_validation, y_validation),
    callbacks=[early_stop_cb]
)

import requests
from PIL import Image
from io import BytesIO

# Input for the image URL
image_url = input("Please enter the URL of the image: ")

try:
    # Download the image from the URL
    response = requests.get(image_url)
    response.raise_for_status()  # Raise an exception for bad status codes
    img = Image.open(BytesIO(response.content)).convert('RGB')

    # Preprocess the image (resize, convert to array, normalize, reshape)
    img_resized = img.resize((64, 64))
    img_array = np.array(img_resized)
    img_processed = img_array / 255.0
    img_processed = img_processed.reshape(1, 64, 64, 3)

    # Make predictions with both models
    prediction_prev = model.predict(img_processed)
    predicted_class_prev = np.argmax(prediction_prev)

    prediction_modern = model_modern.predict(img_processed)
    predicted_class_modern = np.argmax(prediction_modern)


    # Display the image
    plt.imshow(img_array)
    plt.title("Input Image")
    plt.axis('off')
    plt.show()

    # Display predictions from both models
    print(f"Previous model predicted class: {predicted_class_prev}")
    print(f"Modern model predicted class: {predicted_class_modern}")

except requests.exceptions.RequestException as e:
    print(f"Error downloading image: {e}")
except Exception as e:
    print(f"An error occurred: {e}")

